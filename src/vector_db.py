import chromadb
from rank_bm25 import BM25Okapi
import numpy as np
from .config import VENUE_ID_COL

class DenseSparseIndex:
    def __init__(self, df_processed, client_path="./chroma_db"):
        self.df = df_processed
        self.client = chromadb.PersistentClient(path=client_path)
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì»¬ë ‰ì…˜ ìƒì„±/ë¡œë“œ
        self.collection = self.client.get_or_create_collection(
            name="wedding_collection", 
            metadata={"hnsw:space": "cosine"}
        )

    def build_chroma_db(self):
        """Parquet ë°ì´í„°ë¥¼ ChromaDBì— ì ì¬"""
        print("\n[Step 2-1] ChromaDBì— ë°ì´í„° ì ì¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        ids, embeddings, metadatas = [], [], []
        print(f"ğŸ“Š í˜„ì¬ DF ì»¬ëŸ¼ ëª©ë¡: {self.df.columns.tolist()}")
        
        for idx, row in self.df.iterrows():
            doc_id = f"{row[VENUE_ID_COL]}_{row['aspect']}_{idx}"
            ids.append(doc_id)
            embeddings.append(row['vector'].tolist())
            metadatas.append({
                "hall_id": str(row[VENUE_ID_COL]),
                "hall_name": row['hall_name'],
                "aspect": row['aspect'],
                "text": row['text_chunk']
            })
            
            
        BATCH_SIZE = 512 
        total_len = len(ids)
        
        print(f"ğŸ“Š ì´ {total_len}ê°œì˜ ë°ì´í„°ë¥¼ {BATCH_SIZE}ê°œì”© ë‚˜ëˆ„ì–´ ì ì¬í•©ë‹ˆë‹¤.")
        
        for i in range(0, total_len, BATCH_SIZE):
            end_idx = min(i + BATCH_SIZE, total_len)
            
            # ìŠ¬ë¼ì´ì‹± (ië¶€í„° end_idxê¹Œì§€)
            batch_ids = ids[i:end_idx]
            batch_embeddings = embeddings[i:end_idx]
            batch_metadatas = metadatas[i:end_idx]
            
            # upsertë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ì¬
            self.collection.upsert(
                ids=batch_ids,
                embeddings=batch_embeddings,
                metadatas=batch_metadatas
            )
            
            # 512ê°œ ë‹¨ìœ„ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥
            if end_idx % (BATCH_SIZE * 2) == 0 or end_idx == total_len:
                print(f"   - Progress: {end_idx}/{total_len} ì ì¬ ì™„ë£Œ")

        print(f"âœ… ChromaDB êµ¬ì¶• ì™„ë£Œ: ì´ {total_len}ê°œì˜ ë¦¬ë·° êµ¬ì ˆì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        
    def get_dense_hits(self, query, aspect, embedder_model, n_results=50):
        """BGE-M3 ë²¡í„°ë¥¼ ì´ìš©í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Dense)"""
        print(f"  - [{aspect}] ì¸¡ë©´ Dense ê²€ìƒ‰ ì¤‘...")
        query_vector = embedder_model.embed_texts([query])[0]
        results = self.collection.query(
            query_embeddings=[query_vector],
            where={"aspect": aspect}, # íŠ¹ì • ì»¬ëŸ¼ë§Œ í•„í„°ë§
            n_results=n_results
        )
        ranks = {res_id: i + 1 for i, res_id in enumerate(results['ids'][0])}
        meta_map = {res_id: meta for res_id, meta in zip(results['ids'][0], results['metadatas'][0])}
        return ranks, meta_map

    def get_sparse_hits(self, query, aspect, n_results=50):
            """BM25ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (Sparse)"""
            print(f"  - [{aspect}] ì¸¡ë©´ Sparse ê²€ìƒ‰ ì¤‘...")
            
            # [ìˆ˜ì •] reset_indexë¥¼ í•˜ì§€ ì•Šì•„ì•¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì˜ idxë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            target_df = self.df[self.df['aspect'] == aspect] 
            if target_df.empty: return {}
            
            tokenized_corpus = [doc.split() for doc in target_df['text_chunk'].tolist()]
            bm25 = BM25Okapi(tokenized_corpus)
            
            scores = bm25.get_scores(query.split())
            top_indices_in_target = np.argsort(scores)[::-1][:n_results]
            
            ranks = {}
            for rank, i in enumerate(top_indices_in_target):
                # target_df.index[i]ë¥¼ í†µí•´ ì›ë³¸ì˜ ê³ ìœ  index(idx)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                actual_idx = target_df.index[i]
                venue_id = target_df.iloc[i][VENUE_ID_COL]
                
                # [ìˆ˜ì •] build_chroma_dbì™€ ë™ì¼í•œ í¬ë§·ì˜ ID ìƒì„±
                doc_id = f"{venue_id}_{aspect}_{actual_idx}"
                ranks[doc_id] = rank + 1
                
            return ranks
        
    def calculate_rrf(self, d_ranks, s_ranks, metadata_map, k=60, top_n=50):
        """Denseì™€ Sparseì˜ ìˆœìœ„ë¥¼ ê²°í•© (RRF)"""
        all_ids = set(d_ranks.keys()) | set(s_ranks.keys())
        rrf_list = []
        
        for doc_id in all_ids:
            # ê¸°ë³¸ ìˆœìœ„ë¥¼ í¬ê²Œ ì¡ì•„ì„œ(100) ê²°ê³¼ì— ì—†ëŠ” ê²½ìš° ì ìˆ˜ë¥¼ ë‚®ê²Œ ì¤Œ
            d_rank = d_ranks.get(doc_id, 100)
            s_rank = s_ranks.get(doc_id, 100)
            
            score = (1 / (k + d_rank)) + (1 / (k + s_rank))
            
            # [ì²´í¬] metadata_mapì€ Dense ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì±„ì›Œì§€ë¯€ë¡œ, 
            # Sparseì—ì„œë§Œ ë‚˜ì˜¨ ê²°ê³¼ëŠ” ì›ë³¸ dfì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì™€ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            meta = metadata_map.get(doc_id)
            
            if not meta:
                # Sparseì—ì„œë§Œ ë°œê²¬ëœ ê²½ìš° ì›ë³¸ dfì—ì„œ ì •ë³´ ì¶”ì¶œ (ì•ˆì „ì¥ì¹˜)
                try:
                    # doc_id í˜•íƒœ: "1001_catering_58" -> ë§ˆì§€ë§‰ ìˆ«ìê°€ idx
                    original_idx = int(doc_id.split('_')[-1])
                    row = self.df.loc[original_idx]
                    meta = {
                        "hall_id": str(row[VENUE_ID_COL]),
                        "hall_name": row['hall_name'],
                        "aspect": row['aspect'],
                        "text": row['text_chunk']
                    }
                except:
                    continue

            rrf_list.append({**meta, "rrf_score": score})
        
        if not rrf_list: return []
        
        sorted_res = sorted(rrf_list, key=lambda x: x['rrf_score'], reverse=True)[:top_n]
        print(f"  - [{sorted_res[0]['aspect']}] RRF ì™„ë£Œ: ìƒìœ„ {len(sorted_res)}ê°œ ì¶”ì¶œ")
        return sorted_res